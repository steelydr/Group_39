{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa7e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\dsoni\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\dsoni\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dsoni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\dsoni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\dsoni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dsoni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dsoni\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbbb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.97%\n",
      "Hi human, please tell me your GeniSys user\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Read the intents data from a JSON file\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Preprocess the data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in words]\n",
    "    features = [str(sentiment_scores[i] * len(words[i])) + words[i] for i in range(len(words)) if words[i] not in stop_words]\n",
    "    return \" \".join(features)\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for text in intent[\"text\"]:\n",
    "        corpus.append(preprocess(text))\n",
    "        labels.append(intent[\"intent\"])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = labels\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(score * 100))\n",
    "\n",
    "# Build the chatbot\n",
    "def get_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_vector = vectorizer.transform([input_text]).toarray()\n",
    "    predicted_label = classifier.predict(input_vector)[0]\n",
    "    for intent in intents[\"intents\"]:\n",
    "        if intent[\"intent\"] == predicted_label:\n",
    "            response = intent[\"responses\"][0]\n",
    "            return response\n",
    "    return \"Sorry, I don't understand\"\n",
    "\n",
    "# Test the chatbot\n",
    "input_text = \"hi\"\n",
    "response = get_response(input_text)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c304b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "4/4 [==============================] - 6s 55ms/step - loss: 3.0906 - accuracy: 0.0614\n",
      "Epoch 2/60\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 3.0846 - accuracy: 0.0702\n",
      "Epoch 3/60\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 3.0832 - accuracy: 0.0526\n",
      "Epoch 4/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 3.0734 - accuracy: 0.1053\n",
      "Epoch 5/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 3.0660 - accuracy: 0.0614\n",
      "Epoch 6/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 3.0366 - accuracy: 0.1053\n",
      "Epoch 7/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 3.0403 - accuracy: 0.0614\n",
      "Epoch 8/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 3.0101 - accuracy: 0.1140\n",
      "Epoch 9/60\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 3.0017 - accuracy: 0.1228\n",
      "Epoch 10/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.9900 - accuracy: 0.0965\n",
      "Epoch 11/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 3.0015 - accuracy: 0.0702\n",
      "Epoch 12/60\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 2.9074 - accuracy: 0.1491\n",
      "Epoch 13/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.9058 - accuracy: 0.1053\n",
      "Epoch 14/60\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 2.8141 - accuracy: 0.1316\n",
      "Epoch 15/60\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.8339 - accuracy: 0.1579\n",
      "Epoch 16/60\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 2.7159 - accuracy: 0.1842\n",
      "Epoch 17/60\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 2.7904 - accuracy: 0.1667\n",
      "Epoch 18/60\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 2.7764 - accuracy: 0.1228\n",
      "Epoch 19/60\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.7042 - accuracy: 0.1316\n",
      "Epoch 20/60\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 2.7343 - accuracy: 0.1491\n",
      "Epoch 21/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 2.7068 - accuracy: 0.1579\n",
      "Epoch 22/60\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 2.6463 - accuracy: 0.1579\n",
      "Epoch 23/60\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 2.6234 - accuracy: 0.1754\n",
      "Epoch 24/60\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 2.6360 - accuracy: 0.1754\n",
      "Epoch 25/60\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.6828 - accuracy: 0.1228\n",
      "Epoch 26/60\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 2.5980 - accuracy: 0.1842\n",
      "Epoch 27/60\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 2.5661 - accuracy: 0.1842\n",
      "Epoch 28/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 2.4537 - accuracy: 0.2281\n",
      "Epoch 29/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 2.4188 - accuracy: 0.2018\n",
      "Epoch 30/60\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 2.3933 - accuracy: 0.2632\n",
      "Epoch 31/60\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 2.3800 - accuracy: 0.2544\n",
      "Epoch 32/60\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 2.2891 - accuracy: 0.2544\n",
      "Epoch 33/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.3006 - accuracy: 0.2105\n",
      "Epoch 34/60\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 2.2450 - accuracy: 0.2895\n",
      "Epoch 35/60\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 2.1917 - accuracy: 0.2807\n",
      "Epoch 36/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 2.0365 - accuracy: 0.3333\n",
      "Epoch 37/60\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 2.0705 - accuracy: 0.3333\n",
      "Epoch 38/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.9515 - accuracy: 0.3246\n",
      "Epoch 39/60\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 1.9454 - accuracy: 0.3158\n",
      "Epoch 40/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.8306 - accuracy: 0.3947\n",
      "Epoch 41/60\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 1.8536 - accuracy: 0.3772\n",
      "Epoch 42/60\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 1.9049 - accuracy: 0.3246\n",
      "Epoch 43/60\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 1.8343 - accuracy: 0.3509\n",
      "Epoch 44/60\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 1.7091 - accuracy: 0.4298\n",
      "Epoch 45/60\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.6778 - accuracy: 0.3947\n",
      "Epoch 46/60\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 1.5433 - accuracy: 0.4386\n",
      "Epoch 47/60\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 1.5974 - accuracy: 0.4561\n",
      "Epoch 48/60\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 1.4583 - accuracy: 0.5526\n",
      "Epoch 49/60\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 1.4580 - accuracy: 0.5088\n",
      "Epoch 50/60\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 1.5336 - accuracy: 0.4649\n",
      "Epoch 51/60\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 1.5423 - accuracy: 0.4649\n",
      "Epoch 52/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.4058 - accuracy: 0.5263\n",
      "Epoch 53/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.4704 - accuracy: 0.4386\n",
      "Epoch 54/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.3082 - accuracy: 0.5526\n",
      "Epoch 55/60\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 1.3550 - accuracy: 0.5614\n",
      "Epoch 56/60\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 1.2530 - accuracy: 0.5614\n",
      "Epoch 57/60\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.4215 - accuracy: 0.4912\n",
      "Epoch 58/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.1230 - accuracy: 0.6140\n",
      "Epoch 59/60\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.1869 - accuracy: 0.6053\n",
      "Epoch 60/60\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 1.2406 - accuracy: 0.5702\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 1.7300 - accuracy: 0.5172\n",
      "Accuracy: 51.72%\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "Hello human, please tell me your GeniSys user\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from textblob import TextBlob\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "# Read the intents data from a JSON file\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Preprocess the data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    sentiment_scores = [str(TextBlob(word).sentiment.polarity) for word in words]\n",
    "    features = [sentiment_scores[i] + '*' + str(len(words[i])) for i in range(len(words))] # multiply sentiment by word length\n",
    "    return \" \".join(words + features)\n",
    "\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for text in intent[\"text\"]:\n",
    "        corpus.append(preprocess(text))\n",
    "        labels.append(intent[\"intent\"])\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(corpus)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "y = np.zeros((len(labels), len(set(labels))))\n",
    "for i, label in enumerate(labels):\n",
    "    y[i, list(set(labels)).index(label)] = 1\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size + 1, 64, input_length=X.shape[1]))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(labels)), activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=60, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(score[1] * 100))\n",
    "\n",
    "# Build the chatbot\n",
    "def get_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=X.shape[1])\n",
    "    predicted_label = np.argmax(model.predict(input_seq), axis=-1)[0]\n",
    "    for intent in intents[\"intents\"]:\n",
    "        if list(set(labels))[predicted_label] == intent[\"intent\"]:\n",
    "            response = np.random.choice(intent[\"responses\"])\n",
    "            return response\n",
    "    return \"Sorry, I don't understand\"\n",
    "\n",
    "# Test the chatbot\n",
    "input_text = \"Hello there\"\n",
    "response = get_response(input_text)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d14efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsoni\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.83%\n",
      "Hi human, please tell me your GeniSys user\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the intents data from a JSON file\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Preprocess the data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in words]\n",
    "    features = [str(sentiment_scores[i] * len(words[i])) + words[i] for i in range(len(words)) if words[i] not in stop_words]\n",
    "    return \" \".join(features)\n",
    "\n",
    "# Prepare the data\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for text in intent[\"text\"]:\n",
    "        corpus.append(preprocess(text))\n",
    "        labels.append(intent[\"intent\"])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Define the parameters for Grid Search\n",
    "parameters = {\n",
    "    'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__min_df': [1, 2, 3],\n",
    "    'clf__alpha': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "# Perform Grid Search to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = grid_search.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(score * 100))\n",
    "\n",
    "# Build the chatbot\n",
    "def get_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    predicted_label = grid_search.predict([input_text])[0]\n",
    "    for intent in intents[\"intents\"]:\n",
    "        if intent[\"intent\"] == predicted_label:\n",
    "            response = intent[\"responses\"][0]\n",
    "            return response\n",
    "    return \"Sorry, I don't understand\"\n",
    "\n",
    "# Test the chatbot\n",
    "input_text = \"hi\"\n",
    "response = get_response(input_text)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741d49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
